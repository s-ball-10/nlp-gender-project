# -*- coding: utf-8 -*-
"""gender_NLP_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MuZFWWQ-UEt5csh0KBXM18u94VuywYjl

# Installing packages
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
filePath= '/content/drive/MyDrive/' # change this

import gzip
import json
import pandas as pd
import random
import numpy as np
import torch
import warnings
import re
import itertools
import operator
import pickle
import csv
import matplotlib.pyplot as plt

from collections import defaultdict, Counter
from tqdm.notebook import tqdm
from nltk.util import bigrams
from html import unescape
from string import punctuation
from sklearn.feature_extraction import _stop_words
from sklearn.metrics import classification_report, f1_score, confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GroupShuffleSplit
from sklearn.manifold import TSNE
from sklearn.utils import resample
from sklearn.model_selection import train_test_split

from torch import nn, optim
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

warnings.filterwarnings("ignore", category=FutureWarning)

!pip install feather-format
import feather

! pip install gender-guesser
import gender_guesser.detector as gender

!pip3 install pickle5
import pickle5 as pickle

"""# Loading the Amazon book dataset
This section loads the meta book dataset and extracts the reviews for the selected books. The following steps were taken:
- Load meta data for books
- Extract the books for the categories of interest: *classic, romance* and *science fiction*
- Remove book duplicates and children books from the meta dataset
- Load the reviews that correspond to the selected books and store them in a separate file

"""

# Loading the books meta dataset
meta_data = []
with open('meta_Books.json', 'r') as f:
    for l in f:
        obj = json.loads(l)
        meta_data.append(obj)

len(meta_data)

meta_data[1]

# Extracting relevant categories
relevant_categories = ["Romance", "Romantic Suspense", "Romantic Commedy",
                       "Science Fiction", "Science Fiction &amp; Dystopian",
                       "Classics"]

relevant_data = dict()

for cat in relevant_categories:
    for i,item in enumerate(meta_data):
        if cat in meta_data[i]['category']:
                relevant_data[i]= item

# Store relevant books as dataframe
df_all = pd.DataFrame.from_dict(relevant_data, orient="index")

len(df_all)

# Remove book duplicates
df_all_pure=df_all.drop_duplicates(subset = 'asin', ignore_index=True)

len(df_all_pure)

df_all_pure.head()

# Exclude children's books
df_adults = df_all_pure[~df_all_pure.category.astype(str).str.contains('Children')]

# Exclude books of categories which combine romance and science fiction labels ...
# ... because they mix women and men typcial categories
df_adults = df_adults[~((df_adults['category'].astype(str).str.contains('Romance'))
                      & (df_adults['category'].astype(str).str.contains('Science Fiction')))]

len(df_adults)

# Store dataframe
df_adults.to_pickle("df_adults.pkl")

# Store unique book IDs of selected books
unique_products = list(df_adults['asin'])

# Extracting all reviews of the selected books
pattern = re.compile(rb'(?<=asin\":\s\")\d*(?=\")')
with open('selection_books.json', 'w') as outfile:
    with gzip.open("Books.json.gz", "r") as my_file:
        while True:
            line = list(itertools.islice(my_file, 100000))
            if line:
                for l in line:
                    match = re.search(pattern, l)
                    if match:
                        if str(match[0],'utf-8') in unique_products:
                            outfile.write(str(l, 'utf-8'))
            else:
                break

"""# Preparing the final reviews dataset
This section prepares the final dataset. Steps included are:
- Data preprocessing
- Defining how to predict the gender based on usernames
- Once again narrow down book categories (because otherwise dataset would have been to large)
- Applying cleaning functions to reviews and obtain gender predictions
- Deleting reviews with less than 10 words
- Splitting dataset in training, developement and test set

## Stop words, embeddings and cleaning functions
"""

# Loading stop words (previously provided stopwords and sklearn stopwords)
stops_1 = list()

with open(filePath+'stopwords_formative2.txt', 'r') as fd:
    reader = csv.reader(fd)
    for row in fd:
        stops_1.append(row.replace("\n",""))

stops_2 = list(_stop_words.ENGLISH_STOP_WORDS) + ['t', 's', 'S']
len(stops_2)

# Combine both stop lists and extract unique words
stops_1.extend(stops_2)

stops = set(stops_1)
len(stops)

# Load fastText word embeddings
with open(filePath+'fasttext_vectors.p', 'rb') as f:
    fasttext_vecs = pickle.load(f)

# Tokenise words for being able to count words in original review
# (used later for deleting reviews with less than 10 words)
def tokenise(text):
    text = re.sub("[^a-zA-Z0-9]", ' ', text)
    return [w.lower() for w in text.split()]

# Cleaning function used to prepare review texts for LSTM
def cleaning(text):
    text = re.sub("&nbsp;<a.*?</a>|<a.*?</a>", 'url', text)
    text = text.replace('\n',' ')
    text = text.replace('\t',' ')
    text = re.sub("[^a-zA-Z0-9']", ' ', text)
    text = [w.lower() for w in text.strip().split()]
    return [w for w in text if w in fasttext_vecs]

# Cleaning function used to prepare review texts for Naive Bayes and descriptive analyses
def cleaning_for_descriptives(text):
    text = re.sub("&nbsp;<a.*?</a>|<a.*?</a>", 'url', text) # replace urls in review with 'url'
    text = text.replace('\n',' ')
    text = text.replace('\t',' ')
    text = re.sub('[^a-zA-Z0-9]', ' ', text)
    return [w.lower() for w in text.strip().split() if w.lower() not in stops and w.lower() in fasttext_vecs]

"""*Rationale for determining gender*:
- If a username consists of less than three name parts, predict gender based on the first part
- If a username has three parts or more, extract the first three parts, predict the gender for all parts and determine that if one of the parts is female, the gender is set to female. If there is no female part but a male one, the gender is set to male. This approach is based on the observation that some users abbreviate their first name but use a gender-revealing middle name(s).
- All predictions of the gender-guesser which are NOT female or male are set to "none"
"""

# Determine function with predicts gender based on username

# Loading gender guesser
d = gender.Detector(case_sensitive=False)

def prepare_gender_guesser(names):

    # Splittin usernames and lowering letters
    name = [n.lower() for n in names.split()]

    # Determine gender for usernames with more than or equal to three parts
    if len(name) >= 3:
        new_name_list = name[:2]
        gender_pred = [d.get_gender(i) for i in new_name_list ]
        if 'female' in gender_pred:
            return 'female'
        elif 'male' in gender_pred:
            return 'male'
        else:
            return 'none'

    # Determining gender for usernames with less than three parts
    elif len(name) < 3 and len(name) > 0:
        first_name = d.get_gender(name[0])
        if first_name == 'female':
            return first_name
        elif first_name == 'male':
            return first_name
        else:
            first_name = 'none'
            return first_name
    else:
        first_name = 'none'
        return first_name

"""## Further narrowing down of the book genres and obtaining the final preprocessed dataset"""

# For romance genre reduce to category "romance contemporary"
# For science fiction genre reduce to "Science Fiction &amp; Fantasy"
# (both of them are the biggest categories within the genres)

df_reduced = df_adults[((df_adults['category'].astype(str).str.contains('Classic'))
                        | ((df_adults['category'].astype(str).str.contains('Science Fiction'))
                      & (df_adults['category'].astype(str).str.contains('&amp')))
                        |  (df_adults['category'].astype(str).str.contains('Contemporary')))
                       & ~((df_adults['category'].astype(str).str.contains('Teen'))
                           | (df_adults['category'].astype(str).str.contains('Bible'))
                           | (df_adults['category'].astype(str).str.contains('Anthologies'))
                           | (df_adults['category'].astype(str).str.contains('Space'))
                           | (df_adults['category'].astype(str).str.contains('Short Stories')))]

# Customise the category labels
df_reduced.loc[df_reduced['category'].astype(str).str.contains('Romance'), 'label'] = 'romance'
df_reduced.loc[df_reduced['category'].astype(str).str.contains('Classic'), 'label'] = 'classic'
df_reduced.loc[df_reduced['category'].astype(str).str.contains('Science'), 'label'] = 'science_fiction'

df_reduced.head()

df_reduced['label'].value_counts()

# Load preselected book reviews
i = 0
review_data = []
with open(filePath+'selection_books.json', 'r') as f:
    for l in f:
            obj = json.loads(l)
            review_data.append(obj)

len(review_data)

# Create list of final relevant book IDs
relevant_asin = list(df_reduced['asin'])

# Delete variables that are not of interest to this study
delete_keys = ['overall', 'vote', 'verified', 'style', 'summary',
 'unixReviewTime', 'image']

# Predict gender and create genre-gender labels

numer_undetermined_gender = 0

for i in range(len(review_data)):

    # Store book id
    ident = review_data[i]['asin']
    if (ident in relevant_asin) and ('reviewerName' in review_data[i]) and (review_data[i]['reviewerName']) \
    and ('reviewText' in review_data[i]):

        # Predict gender
        review_data[i]['gender_pred']= prepare_gender_guesser(review_data[i]['reviewerName'])
        if review_data[i]['gender_pred']== 'none':
            numer_undetermined_gender +=1
        elif review_data[i]['gender_pred']!= 'none':

            # Apply clearning functions
            review_data[i]['tokenised']=tokenise(review_data[i]['reviewText'])
            review_data[i]['cleaned_text']=cleaning(review_data[i]['reviewText'])
            review_data[i]['without_stops']= cleaning_for_descriptives(review_data[i]['reviewText'])

            # Store book genre and gender to create label
            cat = df_reduced.loc[df_reduced['asin']== ident, 'label'].iloc[0]
            gen = review_data[i]['gender_pred']
            review_data[i]['true_label'] = cat + '_' + gen

print('The number of usernames for which gender could not be identified is: {}.'.format(numer_undetermined_gender))
print('\nThese are {}% of all reviews.'.format(round(100*(numer_undetermined_gender/len(review_data)), 2)))

# Create reviews dataframe
df_uncleaned_reviews = pd.DataFrame.from_dict(review_data)

# Get dataset with all reviews which have gender predicted
df_reviews_cleaned = df_uncleaned_reviews[(df_uncleaned_reviews.gender_pred != 'none')
                                          & (df_uncleaned_reviews.gender_pred.notna())]

# Remove irrelevant variables
df_reviews_cleaned = df_reviews_cleaned.drop(delete_keys, axis=1)

# Create label id column for LSTM later
label2id = {'classic_female': 0, 'classic_male': 1, 'romance_female': 2,
            'romance_male': 3, 'science_fiction_female': 4, 'science_fiction_male': 5}

df_reviews_cleaned['label_id'] = df_reviews_cleaned['true_label'].map(label2id)

df_reviews_cleaned.head()

# Remove reviews with less than 10 words
def count_words(word_list):
    return len(word_list)

df_reviews_sentences = df_reviews_cleaned[df_reviews_cleaned['tokenised'].map(lambda x: count_words(x) >9)]
print('{}% of the reviews have less than 10 words. \
      I will remove them.'.format(round((1-(len(df_reviews_sentences)/len(df_reviews_cleaned)))*100, 2)))

# Store unique categories of the final dataset
categories = list(df_reviews_cleaned.true_label.unique())
categories

# Train, developement, test split based on group shuffle split to avoid information leakage

train_data = pd.DataFrame()
dev_test_data = pd.DataFrame()
dev_data = pd.DataFrame()
test_data = pd.DataFrame()

# Group shuffle split
gs = GroupShuffleSplit(n_splits=2, test_size=.2, random_state=0)
# Note: test_size here means the proportion of groups which should exclusively be in the test set
gs_dev_test = GroupShuffleSplit(n_splits=2, test_size=.5, random_state=0)

for cat in categories:
    train_index, dev_test_index = next(gs.split(df_reviews_sentences[df_reviews_sentences.true_label == cat],
                                                groups=df_reviews_sentences[df_reviews_sentences.true_label == cat].asin))
    train = df_reviews_sentences[df_reviews_sentences.true_label == cat].iloc[train_index]
    dev_test = df_reviews_sentences[df_reviews_sentences.true_label == cat].iloc[dev_test_index]
    train_data = train_data.append(train)
    dev_test_data=dev_test_data.append(dev_test)

    # Split of dev_test_data into dev_data and test_data:
    dev_index, test_index = next(gs_dev_test.split(dev_test[dev_test.true_label==cat],
                                                   groups=dev_test[dev_test.true_label==cat].asin))
    dev = dev_test[dev_test.true_label==cat].iloc[dev_index]
    test = dev_test[dev_test.true_label==cat].iloc[test_index]
    dev_data = dev_data.append(dev)
    test_data = test_data.append(test)

print(len(df_reviews_sentences))
print(len(train_data))
print(len(dev_data))
print(len(test_data))

train_data.head()

"""# Descriptive analyses of the dataset
This section explores the dataset and prepares the vocabulary for the NB classifier. Steps include:
- Obtaining information about number of reviews and distribution of labels
- Extracting word unigrams and their frequency from the training dataset
- Calculating Jaccard similarities of the classes

## Information about the dataset
"""

# Length of entire dataset
print('In total, I am considering {} reviews.'.format(len(df_reviews_sentences)))

# Distribution of lables in the entire dataset
dict_plot_cat = dict()
for cat in categories:
    dict_plot_cat[cat]=len(df_reviews_sentences[df_reviews_sentences.true_label==cat])/len(df_reviews_sentences)

# Labels distribution plot
fig,ax = plt.subplots(figsize=(20,10), dpi=80)

bar_chart = ax.bar(x=list(dict_plot_cat.keys()),
       height = list(dict_plot_cat.values()))

# Axes formatting and labelling
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_visible(False)
ax.spines['bottom'].set_color('#DDDDDD')
ax.tick_params(bottom=True, left=False, labelsize=15)
ax.set_axisbelow(True)
ax.yaxis.grid(True, color='#EEEEEE')
ax.xaxis.grid(False)
ax.set_ylabel('proportion', labelpad=20, fontsize=15)
ax.set_title('Figure 1: Distribution of labels over the entire dataset', pad=20, fontsize=20,
             color='#333333', weight='bold')

for bar in bar_chart:
    ax.text(
      bar.get_x() + bar.get_width() / 2,
      bar.get_height() + 0.01,
      round(bar.get_height(), 4),
      horizontalalignment='center',
      #color=bar_color,
      color = 'black',
      weight='bold',
      size = 13)

plt.tight_layout()
plt.savefig(filePath+'distribution_labels.pdf', format ='pdf')

# Length of the test dataset
print('The length of the test dataset is: {}'.format(len(test_data)))

# Number of females and males in the test dataset
sum_females = len(test_data[test_data.true_label.str.contains('female')])

# Percentage women in the test dataset
print('The percentage of women in the dataset is: {}%'.format(round(sum_females/len(test_data),4)*100))

# Percentage categories in the test dataset
sum_classic = len(test_data[test_data.true_label.str.contains('classic')])
sum_romance = len(test_data[test_data.true_label.str.contains('romance')])
sum_science = len(test_data[test_data.true_label.str.contains('science')])
print('The fraction of classic reviews in the dataset is: {}%'.format(round(sum_classic/len(test_data),2)*100))
print('The fraction of romance reviews in the dataset is: {}%'.format(round(sum_romance/len(test_data),2)*100))
print('The fraction of science reviews in the dataset is: {}%'.format(round(sum_science/len(test_data),2)*100))

"""## Create vocabulary and calculate Jaccard similarities"""

# Initialise data structure for vocabulary dict (inspired by solutions formative 4)
vocab_unigrams = defaultdict(Counter)

# Create vocabulary
for i,j in zip(train_data.without_stops, train_data.true_label):
    vocab_unigrams[j].update(i)

# Inspect most frequent unigrams
for l in categories:
    print(f'The 30 most frequent unigrams in {l.upper()} books:', vocab_unigrams[l].most_common(30))

# Get number of unigrams for each label in the training data
list_number_of_unigrams = list()
print('Number of unigrams:')
for c_i in categories:
    print(f"{c_i.upper()}:\t {len(vocab_unigrams[c_i])} entries")
    list_number_of_unigrams.append(len(vocab_unigrams[c_i]))

# Creating plot for unigram frequencies
fig,ax = plt.subplots(figsize=(20,10), dpi=80)

bar_chart = ax.bar(x=categories,
       height = list_number_of_unigrams)

# Axes formatting and labelling
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_visible(False)
ax.spines['bottom'].set_color('#DDDDDD')
ax.tick_params(bottom=True, left=False, labelsize=15)
ax.set_axisbelow(True)
ax.yaxis.grid(True, color='#EEEEEE')
ax.xaxis.grid(False)
ax.set_ylabel('frequency', labelpad=20, fontsize=15)
ax.set_title('Figure 2: Number of word unigrams in training dataset ', pad=20, fontsize=20,
             color='#333333', weight='bold')

for bar in bar_chart:
    ax.text(
      bar.get_x() + bar.get_width() / 2,
      bar.get_height() + 100,
      round(bar.get_height(), 4),
      horizontalalignment='center',
      color = 'black',
      weight='bold',
      size = 15)

plt.tight_layout()
plt.savefig(filePath+'frequency_unigrams.pdf', format ='pdf')

# Pairwise Jaccard similarities for the vocabularies of all classes

# Defining function for calculating Jaccard similarity
def jaccard_sim(vocab_1, vocab_2):
    return len(vocab_1.intersection(vocab_2)) / len(vocab_1.union(vocab_2))

jacc_unigrams = defaultdict(Counter)

# Initialise and fill matrix
for l_1 in categories:
    for l_2 in categories:
        jacc_unigrams[l_1][l_2]=jaccard_sim(set(vocab_unigrams[l_1]), set(vocab_unigrams[l_2]))

# Store and display tables
df_jaccard_unigram = pd.DataFrame.from_dict(jacc_unigrams, orient='index',
                                            columns=categories).reindex(index=categories)
df_jaccard_unigram.to_latex(buf = filePath+'jaccard_uni', bold_rows = True,
                            caption='Jaccard similarity unigrams', label='jaccard_uni')
print('Pairwise Jaccard similarity for word unigrams:\n')
display(df_jaccard_unigram)

"""# Naive Bayes Classifier
This section is about training and evaluating the NB classifier on the six-label imbalanced dataset. Steps include:
- Setting up model architecture
- Training and evaluating NB only with smoothing
- Obtaining classification reports for random baselines
- Training and evaluating NB with smoothing and feature selection

### Model with smoothing but **without** feature selection
"""

# Get list of unique unigrams
set_list = list()

for cat in categories:
    all_keys = set(vocab_unigrams[cat])
    set_list.append(all_keys)

unique_words_unigrams = set().union(*set_list)

print('The number of unique unigrams in the training set is: {}'.format(len(unique_words_unigrams)))

# Naive bayes with smoothing

# Train the classifier P(w|c_i)
def naive_bayes_smoothing(vocabulary, cat, smoothing_alpha):

    probabilities = dict()

    for c in cat:
        probabilities[c] = dict()
        for word in unique_words_unigrams:
            if vocabulary[c][word]>0:
                probabilities[c][word]= vocabulary[c][word]
            else:
                probabilities[c][word]= smoothing_alpha
        total = sum(probabilities[c].values())
        probabilities[c]={word: probabilities[c][word]/total for word in probabilities[c]}

    return probabilities

# Estimate P(cat), the probability of class cat, based on the class distribution in the train set
prob_class = dict()
for cat in categories:
    prob_class[cat] = len(train_data[train_data.true_label==cat])/len(train_data)
print(prob_class)

# Algorithm for predicting the labels
def get_nb_predictions(categories, test_data, probabilities, prob_class):

    # Initialize lists for storing ground truth labels and predictions
    labels = list()
    predictions = list()

    # Loop over categories
    for c_i in categories:

        # Loop over test reviews
        for rev in test_data[test_data.true_label==c_i].without_stops:

            # Store ground truth
            labels.append(c_i)

            # For each review, calculate scores for each of the categories
            scores = {'classic_female': 0, 'classic_male': 0, 'romance_female':0, 'romance_male':0,
                      'science_fiction_female':0, 'science_fiction_male':0}
            for word in rev:
                if word in probabilities[c_i]:
                    scores["classic_female"] += np.log(probabilities["classic_female"][word])
                    scores["classic_male"] += np.log(probabilities["classic_male"][word])
                    scores["romance_female"] += np.log(probabilities["romance_female"][word])
                    scores["romance_male"] += np.log(probabilities["romance_male"][word])
                    scores["science_fiction_female"] += np.log(probabilities["science_fiction_female"][word])
                    scores["science_fiction_male"] += np.log(probabilities["science_fiction_male"][word])

            # Class imbalance
            scores["classic_female"] += scores["classic_female"] + np.log(prob_class["classic_female"])
            scores["classic_male"] += scores["classic_male"] + np.log(prob_class["classic_male"])
            scores["romance_female"] += scores["romance_female"] + np.log(prob_class["romance_female"])
            scores["romance_male"] += scores["romance_male"] + np.log(prob_class["romance_male"])
            scores["science_fiction_female"] += scores["science_fiction_female"] + np.log(prob_class["science_fiction_female"])
            scores["science_fiction_male"] += scores["science_fiction_male"] + np.log(prob_class["science_fiction_male"])

            # Use highest score for prediction
            predictions.append(max(scores.items(), key=operator.itemgetter(1))[0])

    return labels, predictions

# Hyperparameter tuning of alpha

alpha_parameter = [0.0000000001]+[x * 0.05 for x in range(1, 21)]

macro_f1_dict = dict()

for a in alpha_parameter:

    # Train
    probs = naive_bayes_smoothing(vocab_unigrams, categories, a)

    # Get predictions on dev set
    labels, predictions = get_nb_predictions(categories, dev_data, probs, prob_class)

    # Calculate and store macro F1 on test set
    macro_f1_dict[a] = f1_score(labels, predictions, average="macro")

best_param, worst_param = max(macro_f1_dict, key=macro_f1_dict.get), min(macro_f1_dict, key=macro_f1_dict.get)
best_f1, worst_f1 = max(macro_f1_dict.values()), min(macro_f1_dict.values())

print(f"Smoothing parameter {best_param} produces the HIGHEST macro F1 on the dev set: {best_f1}")
print(f"Smoothing parameter {worst_param} produces the LOWEST macro F1 on the dev set: {worst_f1}")
print(f"The difference between the highest and lowest macro F1 is {best_f1-worst_f1}.")

# Evalutating model with tuned hyperparameter
smoothing_alpha = 0.05

# Train
probs = naive_bayes_smoothing(vocab_unigrams, categories, smoothing_alpha)

# Get predictions on test set
labels, predictions = get_nb_predictions(categories, test_data, probs, prob_class)

print(classification_report(labels, predictions, digits = 4))

# Initialize confusion matrix
c_matrix = defaultdict(Counter)
id2label={0:'classic_female', 1:'classic_male', 2:'romance_female', 3:'romance_male',
          4:'science_fiction_female', 5:'science_fiction_male'}

# Fill confusion matrix
for t, p in zip(labels, predictions):
    c_matrix[t][p] += 1

# Display confusion matrix
pd.DataFrame.from_dict(c_matrix, orient='index', columns=categories).reindex(index=categories)

"""## Simple random baselines"""

# Baseline for randomly predicting the labels according to their probability in the entire dataset
number_class_pred = list()

# Get number of reviews in each class
for i in dict_plot_cat.values():
    number_class_pred.append(round(i*len(predictions), 0))
print(number_class_pred)

sum(number_class_pred)-len(predictions) # one observation too little

# Generating labels according to their frequency in the dataset
dict_class_prob_pred = dict()

# Create list of lables for each category and store them in dict
for l, j in zip(categories, number_class_pred):
    dict_class_prob_pred[l]=[l]*int(j)

# Create one list out of all sublists
new_pred_class_prob = list()

for i in dict_class_prob_pred.values():
    new_pred_class_prob.append(i)

# Flat list:
new_pred_class_prob = [item for sublist in new_pred_class_prob for item in sublist]

# Add label "science_fiction_female" because it was close to be rounded up and otherwise one label too little (see Distribution plot)
new_pred_class_prob += ['science_fiction_female']

# Randomise list
baseline_class_prob = random.sample(new_pred_class_prob, len(new_pred_class_prob))
len(baseline_class_prob)

print('BASELINE FOR RANDOMLY PREDICTING CLASSES ACCORDING TO THEIR PROBABILITY IN THE ENTIRE DATASET\n')
print(classification_report(labels, baseline_class_prob, zero_division=0, digits = 4))

# Store results
report_random = classification_report(labels, baseline_class_prob, digits = 4, output_dict=True)
df_report_random = pd.DataFrame(report_random).transpose()
df_report_random.to_latex(buf = filePath+'random_report', header = ['Precision', 'Recall', 'F1-score', 'Support'],
                      bold_rows = False, float_format="%.4f",
                      caption='Classification Report Radnom baseline, unbalanced dataset', label='report_random')

# Baseline for correctly predicting genre but randomly predicting gender (according to probability within genre)

baseline_corr_genre_random = np.array(labels)

# Get index of labels in correct label list for each genre and then shuffle within book genre
for substr in ['classic', 'romance', 'science']:
    indx_to_shuffle = np.where([substr in x for x in baseline_corr_genre_random])
    print(indx_to_shuffle)
    baseline_corr_genre_random[indx_to_shuffle] = np.random.permutation(baseline_corr_genre_random[indx_to_shuffle])

print('BASELINE FOR CORRECTLY PREDICTING BOOK GENRE BUT RANDOMLY PREDICTING GENDER WITHIN GENRE\n')
print(classification_report(labels, baseline_corr_genre_random, zero_division=0, digits = 4))

# Store results
report_random_genre = classification_report(labels, baseline_corr_genre_random, digits = 4, output_dict=True)
df_report_random_genre = pd.DataFrame(report_random_genre).transpose()
df_report_random_genre.to_latex(buf = filePath+'random_report_genre',
                                header = ['Precision', 'Recall', 'F1-score', 'Support'],
                                bold_rows = False, float_format="%.4f",
                                caption='Classification Report correct genre random gender, imbalanced dataset',
                                label='report_random')

"""## Naive Bayes with smoothing **and feature selection**"""

# Mutual information

# Create dictionary which counts the number of reviews a word appears in
# (ignoring if it appears more than once in a review)

n_reviews = defaultdict(Counter)

for i,j in zip(train_data.without_stops, train_data.true_label):
    n_reviews[j].update(set(i))

# Get number of reviews per category
n_total = dict()

for cat in categories:
    n_total[cat]= len(train_data[train_data.true_label == cat])
n_total

def mutual_info(word, n_reviews, n_total):

    # Create a matrix: for each class (row), how many tweets did it appear in (col1) and not appear in (col2)
    count_matrix = np.array([[n_reviews["classic_female"][word], n_total["classic_female"]
                              - n_reviews["classic_female"][word]],
                             [n_reviews["classic_male"][word], n_total["classic_male"]
                              - n_reviews["classic_male"][word]],
                             [n_reviews["romance_female"][word], n_total["romance_female"]
                              - n_reviews["romance_female"][word]],
                             [n_reviews["romance_male"][word], n_total["romance_male"]
                              - n_reviews["romance_male"][word]],
                             [n_reviews["science_fiction_female"][word], n_total["science_fiction_female"]
                              - n_reviews["science_fiction_female"][word]],
                             [n_reviews["science_fiction_male"][word], n_total["science_fiction_male"]
                              - n_reviews["science_fiction_male"][word]]])

    # Store the sum of that matrix --> total number of tweets in the training set
    N = count_matrix.sum()

    # Initialise the value we will return
    out = 0

    # Iterate through columns and rows
    for col in range(2):
        for row in range(6):

            if count_matrix[row, col]!= 0:
                out += count_matrix[row, col] * np.log2((N*count_matrix[row, col]) / (count_matrix[row, :].sum() * count_matrix[:, col].sum()))

            # Do not increase MI score if no observation in training data
            else:
                out +=0

    return (1/N) * out

mutual_info_values = sorted([(mutual_info(w, n_reviews, n_total), w) for w in unique_words_unigrams], reverse=True)

print('30 overall most informative features:', [w for mi, w in mutual_info_values][:30])

len(mutual_info_values)

# Function for training the classifier P(w|c_i) with feature selection
def naive_bayes(unique_words_unigrams, vocabulary, cat, smoothing_alpha, top_n, mutual_info_values):

    probabilities = dict()

    top_mu_info = dict([(t[1], t[0]) for t in mutual_info_values[ :top_n]]) #feature selection

    for c in cat:
        probabilities[c] = dict()
        for word in unique_words_unigrams:
            if word in top_mu_info:
                if vocabulary[c][word]>0:
                    probabilities[c][word]= vocabulary[c][word]
                else:
                    probabilities[c][word]= smoothing_alpha
        total = sum(probabilities[c].values())
        probabilities[c]={word: probabilities[c][word]/total for word in probabilities[c]}

    return probabilities

# Hyperparameter tuning for smoothing alpha and for top_n features
top_n_parameter = [1000, 2000, 5000, 10000, 15000, 18000, 20000, 20729]

alpha_parameter = [0.0000000001]+[x * 0.05 for x in range(1, 21)] #maybe later more.

macro_f1_dict = dict()

for n in top_n_parameter:
    for a in alpha_parameter:

        # Train
        probs = naive_bayes(unique_words_unigrams, vocab_unigrams, categories, a, n, mutual_info_values)

        # Get predictions on dev set
        labels, predictions = get_nb_predictions(categories, dev_data, probs, prob_class)

        # Calculate and store macro F1 on test set
        macro_f1_dict[str(n)+'_'+str(a)] = f1_score(labels, predictions, average="macro")

best_param, worst_param = max(macro_f1_dict, key=macro_f1_dict.get), min(macro_f1_dict, key=macro_f1_dict.get)
best_f1, worst_f1 = max(macro_f1_dict.values()), min(macro_f1_dict.values())

print(f"Smoothing parameter {best_param} produces the HIGHEST macro F1 on the dev set: {best_f1}")
print(f"Smoothing parameter {worst_param} produces the LOWEST macro F1 on the dev set: {worst_f1}")
print(f"The difference between the highest and lowest macro F1 is {best_f1-worst_f1}.")

# Train and evaluate on test set
top_n = 20000
smoothing_alpha = 0.05

# Train
probs = naive_bayes(unique_words_unigrams, vocab_unigrams, categories, smoothing_alpha, top_n,  mutual_info_values)

# Get predictions on test set
labels, predictions = get_nb_predictions(categories, test_data, probs, prob_class)

# Get classification report
report_nb = classification_report(labels, predictions, digits = 4, output_dict=True)
df_report_nb = pd.DataFrame(report_nb).transpose()
df_report_nb.to_latex(buf = filePath+'nb_report', header = ['Precision', 'Recall', 'F1-score', 'Support'],
                      bold_rows = False, float_format="%.4f",
                      caption='Classification Report NB classifier, unbalanced dataset', label='report_nb')
print(classification_report(labels, predictions, digits = 4))

# Initialize confusion matrix
c_matrix = defaultdict(Counter)
id2label={0:'classic_female', 1:'classic_male', 2:'romance_female', 3:'romance_male', 4:'science_fiction_female', 5:'science_fiction_male'}

# Fill confusion matrix
for t, p in zip(labels, predictions):
    c_matrix[t][p] += 1

# Display confusion matrix
df_confu_nb = pd.DataFrame.from_dict(c_matrix, orient='index', columns=categories).reindex(index=categories)
display(df_confu_nb)

# Store results as latex file
df_confu_nb.to_latex(buf = filePath+'confu_nb', bold_rows = True,
                     caption='Confusion Matrix NB classifier, unbalanced dataset')

# Function to calculate how often the classifier correctly predicted the gender, irrespective of book genre
def correct_gender_pred(df_confu, test_dataframe):

    #Get number of correct female and male predictions
    sum_female_correct = df_confu.iloc[[0,2,4],[0,2,4]].values.sum()
    sum_male_correct = df_confu.iloc[[1,3,5],[1,3,5]].values.sum()
    length_dataset = len(test_dataframe)

    # Get percentage of correctly predicted gender labels
    percent_corr_pred = round(100*((sum_female_correct + sum_male_correct)/length_dataset),2)

    # Get percentage of labels that contain "female"
    percent_female = round(100*(len(test_dataframe[test_dataframe.true_label.str.contains('female')])
                                /len(test_dataframe)), 2)

    # Compare test set to predictions
    print('The percentage of women in the dataset is: {}%.'.format(percent_female))
    print('The percentage of correctly classified gender independent of category is: {}%.'.format(percent_corr_pred))
    print('This gives a difference of {} percentage points.'.format(abs(round(percent_corr_pred-percent_female,2))))

    return percent_corr_pred

corr_gender_nb = correct_gender_pred(df_confu_nb, test_data)

"""# LSTM classifier

This section implements the LSTM classifier. This includes:
- Setting up the architecture
- Doing hyperparameter tuning
- Train and evaluate on test set

## Setting up LSTM architecture
"""

# Define dataset class
class LSTMDataset(Dataset):
    def __init__(self, data, w2id):

        # Encode reviews
        self.reviews = list(data.cleaned_text.apply(lambda x: [w2id[w] if w in w2id else 1 for w in x]))

        # Store labels
        self.labels = list(data.label_id)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        review = self.reviews[idx]
        label = self.labels[idx]
        return review, label

# Define collate function
def lstm_collate(batch):

    # Store batch size
    batch_size = len(batch)

    # Separate reviews and labels
    reviews = [r for r, _ in batch]
    labels = torch.tensor([l for _, l in batch]).long() # .long means that the dtype is now int64

    # Store length of longest review in batch
    max_len = max(len(r) for r in reviews)

    # Create padded review tensors
    reviews_pad = torch.zeros((batch_size, max_len)).long()
    for i, r in enumerate(reviews):
        reviews_pad[i, :len(r)] = torch.tensor(r)

    return reviews_pad, labels

# Create dictionary for word look-up
word_counter = Counter([w for r in train_data.cleaned_text for w in r])
w2id = {w: i + 2 for i, w in enumerate(w for w, c in word_counter.most_common())}

# Create dictionary for reverse word look-up
id2w = {i: w for w, i in w2id.items()}

# Create datasets
train_dataset = LSTMDataset(train_data, w2id)
dev_dataset = LSTMDataset(dev_data, w2id)
test_dataset = LSTMDataset(test_data, w2id)

# Create matrix of pretrained embeddings
fasttext_emb = torch.tensor(np.array([fasttext_vecs[id2w[i]] for i in range(2, len(id2w) + 2)])).float()
fasttext_emb = torch.cat((torch.zeros((1, 300)), fasttext_emb.mean(axis=0, keepdim=True), fasttext_emb), axis=0)

"""## Hyperparameter tuning LSTM"""

macro_f1_dict_lstm = dict()

b_sizes = [16, 32, 64, 128]
l_rate = [0.01, 0.0015, 0.05, 0.1, 0.15]
d_rate = [0.1, 0.2]


for ba in b_sizes:
  for l in l_rate:
    for d in d_rate:

      # Define LSTM classifier
      class LSTMClassifier(nn.Module):

          def __init__(self, pretrained_emb, hidden_dim, output_dim):

              # Define network layers
              super(LSTMClassifier, self).__init__()
              self.embedding = nn.Embedding.from_pretrained(pretrained_emb, padding_idx=0)
              self.lstm = nn.LSTM(pretrained_emb.shape[1], hidden_dim, batch_first=True)
              self.linear = nn.Linear(hidden_dim, output_dim)

              # Define dropout
              self.dropout = nn.Dropout(d)

          def forward(self, reviews):

              # Define flow of tensors through network
              emb = self.embedding(reviews)
              output, (hidden, cell) = self.lstm(self.dropout(emb))
              return self.linear(self.dropout(output[:, -1, :]))


      # Create data loaders
      batch_size = ba

      train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=lstm_collate, shuffle=True)
      dev_loader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=lstm_collate)
      test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=lstm_collate)

      # Initialize model
      model = LSTMClassifier(fasttext_emb, hidden_dim = 200, output_dim = 6)

      # Define optimizer and training objective
      optimizer = optim.Adam(model.parameters(), lr=l)
      criterion = nn.CrossEntropyLoss()

      # Define device and move model to CUDA if available
      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      model = model.to(device)

      # Train model
      for epoch in range(1, 10):

          model.train()

          for i, b in enumerate(train_loader):

              # Perform forward pass
              optimizer.zero_grad()
              reviews, lbls = [t.to(device) for t in b]
              output = model(reviews)
              loss = criterion(output, lbls)

              # Perform backpropagation and update weights
              loss.backward()
              optimizer.step()

          # Evaluate model on development data
          if epoch % 1 == 0:
            model.eval()

            y_true = list()
            y_pred = list()

            with torch.no_grad():
                for b in dev_loader:
                    reviews, lbls = [t.to(device) for t in b]
                    output = model(reviews)
                    max_output = output.argmax(dim=1)
                    y_true.extend(lbls.tolist())
                    y_pred.extend(max_output.tolist())


            # Calculate and store macro F1 on test set
            macro_f1_dict_lstm[str(ba)+'_'+str(l)+'_'+str(d)+'_'+str(epoch)] = f1_score(y_true, y_pred, average="macro")

best_param_lstm = max(macro_f1_dict_lstm, key=macro_f1_dict_lstm.get)
print(f"The parameters {best_param_lstm} produce the HIGHEST macro F1 on the dev set")

"""## LSTM with optimal hyperparameter"""

# Define LSTM classifier
class LSTMClassifier(nn.Module):

    def __init__(self, pretrained_emb, hidden_dim, output_dim):

        # Define network layers
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(pretrained_emb, padding_idx=0)
        self.lstm = nn.LSTM(pretrained_emb.shape[1], hidden_dim, batch_first=True)
        self.linear = nn.Linear(hidden_dim, output_dim)

        # Define dropout
        self.dropout = nn.Dropout(0.2)

    def forward(self, reviews):

        # Define flow of tensors through network
        emb = self.embedding(reviews)
        output, (hidden, cell) = self.lstm(self.dropout(emb))
        return self.linear(self.dropout(output[:, -1, :]))

# Create data loaders
batch_size = 16

train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=lstm_collate, shuffle=True)
dev_loader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=lstm_collate)
test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=lstm_collate)

# Initialize model
model = LSTMClassifier(fasttext_emb, hidden_dim = 200, output_dim = 6)

# Define optimizer and training objective
optimizer = optim.Adam(model.parameters(), lr=0.0015)
criterion = nn.CrossEntropyLoss()

# Define device and move model to CUDA if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Train model
for epoch in range(1, 6):

    model.train()

    for i, b in enumerate(tqdm(train_loader)):

        # Perform forward pass
        optimizer.zero_grad()
        reviews, lbls = [t.to(device) for t in b]
        output = model(reviews)
        loss = criterion(output, lbls)

        # Perform backpropagation and update weights
        loss.backward()
        optimizer.step()

    # Evaluate model on development data
    # Evaluate model every fifth epoch
    if epoch % 1 == 0:
      model.eval()

      y_true = list()
      y_pred = list()

      with torch.no_grad():
          for b in dev_loader: #change back to dev_loader!!!
              reviews, lbls = [t.to(device) for t in b]
              output = model(reviews)
              max_output = output.argmax(dim=1)
              y_true.extend(lbls.tolist())
              y_pred.extend(max_output.tolist())

      print('Development accuracy after {} epoch(s): {:.2f}'.format(epoch, accuracy_score(y_true, y_pred)))
      print('Macro F1 score after {} epoch(s): {:.2f}'.format(epoch, f1_score(y_true, y_pred, average="macro")))

# Evaluate model on test data
model.eval()

y_true = list()
y_pred = list()

with torch.no_grad():
    for b in test_loader:
        reviews, lbls = [t.to(device) for t in b]
        output = model(reviews)
        max_output = output.argmax(dim=1)
        y_true.extend(lbls.tolist())
        y_pred.extend(max_output.tolist())

print('Test accuracy: {:.2f}'.format(accuracy_score(y_true, y_pred)))

# Get classification report
report_lstm = classification_report(y_true, y_pred, digits = 4, output_dict=True)
df_report_lstm = pd.DataFrame(report_lstm).transpose()
df_report_lstm.to_latex(buf = filePath+'lstm_report', header = ['Precision', 'Recall', 'F1-score', 'Support'],
                      bold_rows = False, float_format="%.4f",
                      caption='Classification Report LSTM classifier, unbalanced dataset', label='report_lstm')

print(classification_report(y_true, y_pred, digits = 4))

# Initialize confusion matrix
c_matrix = defaultdict(Counter)
id2label={0:'classic_female', 1:'classic_male', 2:'romance_female', 3:'romance_male',
          4:'science_fiction_female', 5:'science_fiction_male'}

# Fill confusion matrix
for t, p in zip(y_true, y_pred):
    c_matrix[id2label[t]][id2label[p]] += 1

# Display confusion matrix
df_confu_lstm = pd.DataFrame.from_dict(c_matrix, orient='index',
                                       columns=id2label.values()).reindex(index=id2label.values())
df_confu_lstm.fillna(0, inplace=True) # NaN values occur if there was no prediction for a specific label
display(df_confu_lstm)

# Store results as latex file
df_confu_lstm.to_latex(buf = filePath+'confu_lstm', bold_rows = True,
                       caption='Confusion Matrix LSTM classifier, unbalanced dataset')

# Accuracy of correctly predicting gender irrespective of book genre
corr_gender_lstm = correct_gender_pred(df_confu_lstm, test_data)

"""# Models with balanced datasets
This section implements the second data structure setup, i.e. obtaining three gender-specific balanced datasets and evaluate NB on them. Steps inlcude:
- Up- and downsampling
- Train-test split for the separate datasets
- Adjust model architecture of NB to two label classification task
- Training and evaluating NB classifiers on the separate datasets

## Balancing and splitting the dataset
"""

# Up- and Downsampling

# Splitting dataset by label
df_classic_female= df_reviews_sentences[df_reviews_sentences['true_label']=='classic_female']
df_classic_male= df_reviews_sentences[df_reviews_sentences['true_label']=='classic_male']
df_romance_female= df_reviews_sentences[df_reviews_sentences['true_label']=='romance_female']
df_romance_male = df_reviews_sentences[df_reviews_sentences['true_label']=='romance_male']
df_science_female = df_reviews_sentences[df_reviews_sentences['true_label']=='science_fiction_female']
df_science_male = df_reviews_sentences[df_reviews_sentences['true_label']=='science_fiction_male']

dict_len_dataset = {'classic_female': len(df_classic_female), 'classic_male':len(df_classic_male),
                    'romance_female':len(df_romance_female), 'romance_male': len(df_romance_male),
                    'science_fiction_female': len(df_science_female), 'science_fiction_male': len(df_science_male)}

length_shortest_df = dict_len_dataset[min(dict_len_dataset, key=dict_len_dataset.get)]
length_longest_df = dict_len_dataset[max(dict_len_dataset, key=dict_len_dataset.get)]
print('Shortest dataset: ', min(dict_len_dataset, key=dict_len_dataset.get))
print('Longest dataset: ', max(dict_len_dataset, key=dict_len_dataset.get))

# Upsampling
df_cl_f_up = resample(df_classic_female,random_state=42,n_samples= length_longest_df ,replace=True)
df_cl_m_up = resample(df_classic_male,random_state=42,n_samples= length_longest_df ,replace=True)
df_r_f_up = resample(df_romance_female,random_state=42,n_samples= length_longest_df ,replace=True)
df_r_m_up = resample(df_romance_male,random_state=42,n_samples= length_longest_df ,replace=True)
df_s_f_up = resample(df_science_female,random_state=42,n_samples= length_longest_df ,replace=True)
df_s_m_up = resample(df_science_male,random_state=42,n_samples= length_longest_df ,replace=True)

# Downsampling
df_cl_f_down = resample(df_classic_female,random_state=42,n_samples= length_shortest_df ,replace=True)
df_cl_m_down = resample(df_classic_male,random_state=42,n_samples= length_shortest_df ,replace=True)
df_r_f_down = resample(df_romance_female,random_state=42,n_samples= length_shortest_df ,replace=True)
df_r_m_down = resample(df_romance_male,random_state=42,n_samples= length_shortest_df ,replace=True)
df_s_f_down = resample(df_science_female,random_state=42,n_samples= length_shortest_df ,replace=True)
df_s_m_down = resample(df_science_male,random_state=42,n_samples= length_shortest_df ,replace=True)


print('Length of one downsampled dataset: {}'.format(len(df_cl_f_down)))
print('Length of one upsampled dataset: {}'.format(len(df_cl_f_up)))

# Generating separate genre-specific datasets for NB balanced

# Downsampled
df_classic = pd.concat([df_cl_m_down, df_cl_f_down])
df_romance = pd.concat([df_r_f_down, df_r_m_down])
df_science = pd.concat([df_s_f_down, df_s_m_down])

print(len(df_classic), len(df_romance), len(df_science))

# Upsampled
df_classic_up = pd.concat([df_cl_m_up, df_cl_f_up])
df_romance_up = pd.concat([df_r_f_up, df_r_m_up])
df_science_up = pd.concat([df_s_f_up, df_s_m_up])

print(len(df_classic_up), len(df_romance_up), len(df_science_up))

# Split data into training, development, and test sets for two label dataset

# Upsampled
train_classic_up, dev_test_classic_up = train_test_split(df_classic_up, test_size=0.2,
                                                         stratify=df_classic_up['true_label'], random_state=0)
dev_classic_up, test_classic_up = train_test_split(dev_test_classic_up, test_size=0.5,
                                                   stratify=dev_test_classic_up['true_label'], random_state=0)

train_romance_up, dev_test_romance_up = train_test_split(df_romance_up, test_size=0.2,
                                                         stratify=df_romance_up['true_label'], random_state=0)
dev_romance_up, test_romance_up = train_test_split(dev_test_romance_up, test_size=0.5,
                                                   stratify=dev_test_romance_up['true_label'], random_state=0)

train_science_up, dev_test_science_up = train_test_split(df_science_up, test_size=0.2,
                                                         stratify=df_science_up['true_label'], random_state=0)
dev_science_up, test_science_up = train_test_split(dev_test_science_up, test_size=0.5,
                                                   stratify=dev_test_science_up['true_label'], random_state=0)

# Downsampled
train_classic, dev_test_classic = train_test_split(df_classic, test_size=0.2,
                                                   stratify=df_classic['true_label'], random_state=0)
dev_classic, test_classic = train_test_split(dev_test_classic, test_size=0.5,
                                             stratify=dev_test_classic['true_label'], random_state=0)

train_romance, dev_test_romance = train_test_split(df_romance, test_size=0.2,
                                                   stratify=df_romance['true_label'], random_state=0)
dev_romance, test_romance = train_test_split(dev_test_romance, test_size=0.5,
                                             stratify=dev_test_romance['true_label'], random_state=0)

train_science, dev_test_science = train_test_split(df_science, test_size=0.2,
                                                   stratify=df_science['true_label'], random_state=0)
dev_science, test_science = train_test_split(dev_test_science, test_size=0.5,
                                             stratify=dev_test_science['true_label'], random_state=0)

print(len(train_classic_up), len(train_romance_up), len(train_science_up))

print(len(test_classic_up), len(test_romance_up), len(test_science_up))

print(len(dev_classic_up), len(dev_romance_up), len(dev_science_up))

print(len(train_classic), len(train_romance), len(train_science))

"""## Adapting NB architecture to two-label datasets

"""

def get_nb_predictions_bi(cat_red, test_data, probabilities, prob_class):

    # Initialize lists for storing ground truth labels and predictions
    labels = list()
    predictions = list()

    # Loop over categories
    for c_i in cat_red:

        # Loop over test reviews
        for rev in test_data[test_data.true_label==c_i].without_stops:

            # Store ground truth
            labels.append(c_i)

            # For each review, calculate scores for each of the categories
            scores = {cat_red[0]: 0, cat_red[1]: 0}
            for word in rev:
                if word in probabilities[c_i]:
                    scores[cat_red[0]] += np.log(probabilities[cat_red[0]][word])
                    scores[cat_red[1]] += np.log(probabilities[cat_red[1]][word])

            # Class imbalance
            scores[cat_red[0]] += scores[cat_red[0]] + np.log(prob_class[cat_red[0]])
            scores[cat_red[1]] += scores[cat_red[1]] + np.log(prob_class[cat_red[1]])

            # Use highest score for prediction
            predictions.append(max(scores.items(), key=operator.itemgetter(1))[0])

    return labels, predictions

# Define function to obtain vocabulary and  mutual info for a specific training dataset

def vocab_mutual_info_bi(train_df, cat_red):

    vocab_unigrams = defaultdict(Counter)

    # Create vocabulary
    for i,j in zip(train_df.without_stops, train_df.true_label):
        vocab_unigrams[j].update(i)

    # get list of unique unigrams
    set_list = list()

    for cat in cat_red:
        all_keys = set(vocab_unigrams[cat])
        set_list.append(all_keys)

    unique_words_unigrams = set().union(*set_list)


    # Mutual information

    # Create dictionary which counts the number of reviews a word appears in

    n_reviews = defaultdict(Counter)

    for i,j in zip(train_df.without_stops, train_df.true_label):
        n_reviews[j].update(set(i))

    n_total = dict() # number of reviews per category

    for cat in cat_red:
        n_total[cat]= len(train_df[train_df.true_label == cat])


    def mutual_info(word, n_reviews, n_total):

        # Create a matrix: for each class (row), how many tweets did it appear in (col1) and not appear in (col2)
        count_matrix = np.array([[n_reviews[cat_red[0]][word], n_total[cat_red[0]] - n_reviews[cat_red[0]][word]],
                                 [n_reviews[cat_red[1]][word], n_total[cat_red[1]] - n_reviews[cat_red[1]][word]]])

        # Store the sum of that matrix --> total number of tweets in the training set
        N = count_matrix.sum()

        # Initialise the value we will return
        out = 0

        # Iterate through columns and rows
        for col in range(2):
            for row in range(2):

                if count_matrix[row, col]!= 0:
                    out += count_matrix[row, col] * np.log2((N*count_matrix[row, col]) / (count_matrix[row, :].sum() * count_matrix[:, col].sum()))

                # Mutual information set to zero if word not in class
                else:
                    out =0

        return (1/N) * out

    mutual_info_values = sorted([(mutual_info(w, n_reviews, n_total), w) for w in unique_words_unigrams], reverse=True)

    # Class probabilities (not necessary)
    prob_class = dict()
    for cat in cat_red:
        prob_class[cat] = len(train_df[train_df.true_label==cat])/len(train_df)

    return mutual_info_values, vocab_unigrams, prob_class, unique_words_unigrams

# Define function for hyperparamter tuning

top_n_parameter = [1000, 2000, 5000, 10000, 20000, 25000, 50000]

alpha_parameter = [0.0000000001]+[x * 0.05 for x in range(1, 21)]

def hyperparameter_tuning_bi_nb(unique_words_unigrams, dev_data, vocab, cat_red, mutual_info_values, prob_class):

    macro_f1_dict = dict()

    for n in top_n_parameter:
        for a in alpha_parameter:

            # Train
            probs = naive_bayes(unique_words_unigrams, vocab, cat_red, a, n, mutual_info_values)

            # Get predictions on dev set
            labels, predictions = get_nb_predictions_bi(cat_red, dev_data, probs, prob_class)

            # Calculate and store macro F1 on test set
            macro_f1_dict[str(n)+'_'+str(a)] = f1_score(labels, predictions, average="macro")

    best_param, worst_param = max(macro_f1_dict, key=macro_f1_dict.get), min(macro_f1_dict, key=macro_f1_dict.get)
    best_f1, worst_f1 = max(macro_f1_dict.values()), min(macro_f1_dict.values())

    print(f"\nSmoothing parameter {best_param} produces the HIGHEST macro F1 on the dev set: {best_f1}")
    print(f"Smoothing parameter {worst_param} produces the LOWEST macro F1 on the dev set: {worst_f1}")
    print(f"The difference between the highest and lowest macro F1 is {best_f1-worst_f1}.")
    return best_param

# Define function to evaluate NB on test set
def eval_nb_bi(best_n, best_a, unique_words_unigrams, vocab_unigrams, cat_red, mutual_info_values,test_data, prob_class):

    # Hyperparameter
    top_n = best_n
    smoothing_alpha = best_a

    # Train
    probs = naive_bayes(unique_words_unigrams, vocab_unigrams, cat_red, smoothing_alpha, top_n,  mutual_info_values)

    # Get predictions on test set
    labels, predictions = get_nb_predictions_bi(cat_red, test_data, probs, prob_class)

    return labels,predictions

# Define function to obtain confusion matrix
def confu_matrix(cat_red, y_true, y_pred):
    # Initialize confusion matrix
    c_matrix = defaultdict(Counter)

    # Fill confusion matrix
    for t, p in zip(y_true, y_pred):
        c_matrix[t][p] += 1

    # Create confusion matrix dataframe
    confu_df = pd.DataFrame.from_dict(c_matrix, orient='index', columns=cat_red).reindex(index=cat_red)

    return confu_df

"""## Train and evaluate NB on upsampled dataset"""

# Get vocabulary, MI for each genre and perform hyperparameter tuning

# classic books
cat_red = ['classic_female', 'classic_male']
mutual_info_class, unigrams_class, prob_class_class, unique_class = vocab_mutual_info_bi(train_classic_up, cat_red)
hyperpara_class = hyperparameter_tuning_bi_nb(unique_class,dev_classic_up, unigrams_class,
                                              cat_red, mutual_info_class, prob_class_class)
print('Best hyperparameter classic reviews: {}'.format(hyperpara_class))

# romance books
cat_red = ['romance_female', 'romance_male']
mutual_info_romance, unigrams_romance, prob_class_romance, unique_romance = vocab_mutual_info_bi(train_romance_up, cat_red)
hyperpara_romance = hyperparameter_tuning_bi_nb(unique_romance, dev_romance_up, unigrams_romance,
                                                cat_red, mutual_info_romance, prob_class_romance)
print('Best hyperparameter romance reviews: {}'.format(hyperpara_romance))

# science fiction books
cat_red = ['science_fiction_female', 'science_fiction_male']
mutual_info_science, unigrams_science, prob_class_science, unique_science = vocab_mutual_info_bi(train_science_up, cat_red)
hyperpara_science = hyperparameter_tuning_bi_nb(unique_science, dev_science_up, unigrams_science,
                                                cat_red, mutual_info_science, prob_class_science)
print('Best hyperparameter science fiction reviews: {}'.format(hyperpara_science))

# Get predictions and evaluations on test set

# classic books
n = 20000
a = 0.0000000001
labels_c, pred_c = eval_nb_bi(n, a, unique_class, unigrams_class, ['classic_female', 'classic_male'],
                              mutual_info_class, test_classic_up, prob_class_class)

confu_c = confu_matrix(['classic_female', 'classic_male'], labels_c, pred_c)
confu_c.to_latex(buf = filePath+'confu_c_up', bold_rows = True,
                 caption='Confusion Matrix NB classifier, balanced classic dataset (up)', label='confu_nb_c_up')

print('CLASSIFICATION RESULTS FOR CLASSIC REVIEWS DATASET: ')
print(classification_report(labels_c, pred_c, digits = 4))
report_nb_c = classification_report(labels_c, pred_c, digits = 4, output_dict=True)
df_report_nb_c = pd.DataFrame(report_nb_c).transpose()
df_report_nb_c.to_latex(buf = filePath+'nb_report_c_up', header = ['Precision', 'Recall', 'F1-score', 'Support'],
                      bold_rows = False, float_format="%.4f",
                      caption='Classification Report NB classifier, balanced classic dataset (up)',
                        label='report_nb_c_up')
print('\nConfusion matrix:')
display(confu_c)

# romance books
n = 20000
a = 0.0000000001
labels_r, pred_r = eval_nb_bi(n, a, unique_romance, unigrams_romance, ['romance_female', 'romance_male'],
                              mutual_info_romance, test_romance_up, prob_class_romance)

confu_r = confu_matrix(['romance_female', 'romance_male'], labels_r, pred_r)
confu_r.to_latex(buf = filePath+'confu_r_up', bold_rows = True,
                 caption='Confusion Matrix NB classifier, balanced romance dataset (up)', label='confu_nb_r_up')

print('CLASSIFICATION RESULTS FOR ROMANCE REVIEWS DATASET: ')
print(classification_report(labels_r, pred_r, digits = 4))
report_nb_r = classification_report(labels_r, pred_r, digits = 4, output_dict=True)
df_report_nb_r = pd.DataFrame(report_nb_r).transpose()
df_report_nb_r.to_latex(buf = filePath+'nb_report_r_up', header = ['Precision', 'Recall', 'F1-score', 'Support'],
                      bold_rows = False, float_format="%.4f",
                      caption='Classification Report NB classifier, balanced romance dataset (up)',
                        label='report_nb_r_up')
print('\nConfusion matrix:')
display(confu_r)

# science fiction books
n = 20000
a = 0.0000000001
labels_s, pred_s = eval_nb_bi(n, a, unique_science, unigrams_science,
                              ['science_fiction_female', 'science_fiction_male'], mutual_info_science,
                              test_science_up, prob_class_science)

confu_s = confu_matrix(['science_fiction_female', 'science_fiction_male'], labels_s, pred_s)
confu_s.to_latex(buf = filePath+'confu_s_up', bold_rows = True,
                 caption='Confusion Matrix NB classifier, balanced science dataset (up)', label='confu_nb_s_up')

print('CLASSIFICATION RESULTS FOR SCIENCE FICTION REVIEWS DATASET: ')
print(classification_report(labels_s, pred_s, digits = 4))
report_nb_s = classification_report(labels_s, pred_s, digits = 4, output_dict=True)
df_report_nb_s = pd.DataFrame(report_nb_s).transpose()
df_report_nb_s.to_latex(buf = filePath+'nb_report_s_up', header = ['Precision', 'Recall', 'F1-score', 'Support'],
                      bold_rows = False, float_format="%.4f",
                      caption='Classification Report NB classifier, balanced science dataset (up)',
                        label='report_nb_s_up')
print('\nConfusion matrix:')
display(confu_s)

"""## Train and evaluate NB on downsampled dataset"""

# Get vocabulary, MI for each genre and perform hyperparameter tuning

# classic books
cat_red = ['classic_female', 'classic_male']
mutual_info_class, unigrams_class, prob_class_class, unique_class = vocab_mutual_info_bi(train_classic, cat_red)
hyperpara_class = hyperparameter_tuning_bi_nb(unique_class,dev_classic,
                                              unigrams_class, cat_red, mutual_info_class, prob_class_class)
print('Best hyperparameter classic reviews: {}'.format(hyperpara_class))

# romance books
cat_red = ['romance_female', 'romance_male']
mutual_info_romance, unigrams_romance, prob_class_romance, unique_romance = vocab_mutual_info_bi(train_romance, cat_red)
hyperpara_romance = hyperparameter_tuning_bi_nb(unique_romance, dev_romance,
                                                unigrams_romance, cat_red, mutual_info_romance, prob_class_romance)
print('Best hyperparameter romance reviews: {}'.format(hyperpara_romance))

# science fiction books
cat_red = ['science_fiction_female', 'science_fiction_male']
mutual_info_science, unigrams_science, prob_class_science, unique_science = vocab_mutual_info_bi(train_science, cat_red)
hyperpara_science = hyperparameter_tuning_bi_nb(unique_science, dev_science,
                                                unigrams_science, cat_red, mutual_info_science, prob_class_science)
print('Best hyperparameter science fiction reviews: {}'.format(hyperpara_science))

# Get predictions on test set

# classic books
n = 20000
a = 0.0000000001
labels_c, pred_c = eval_nb_bi(n, a, unique_class, unigrams_class, ['classic_female', 'classic_male'],
                              mutual_info_class, test_classic, prob_class_class)

confu_c = confu_matrix(['classic_female', 'classic_male'], labels_c, pred_c)
confu_c.to_latex(buf = filePath+'confu_c_down', bold_rows = True,
                 caption='Confusion Matrix NB classifier, balanced classic dataset')

print('CLASSIFICATION RESULTS FOR CLASSIC REVIEWS DATASET: ')
print(classification_report(labels_c, pred_c, digits = 4))
report_nb_c = classification_report(labels_c, pred_c, digits = 4, output_dict=True)
df_report_nb_c = pd.DataFrame(report_nb_c).transpose()
df_report_nb_c.to_latex(buf = filePath+'nb_report_c', header = ['Precision', 'Recall', 'F1-score', 'Support'],
                      bold_rows = False, float_format="%.4f",
                      caption='Classification Report NB classifier, balanced classic dataset', label='report_nb_c')
print('\nConfusion matrix:')
display(confu_c)

# romance books
n = 20000
a = 0.3
labels_r, pred_r = eval_nb_bi(n, a, unique_romance, unigrams_romance, ['romance_female', 'romance_male'],
                              mutual_info_romance, test_romance, prob_class_romance)
confu_r = confu_matrix(['romance_female', 'romance_male'], labels_r, pred_r)
confu_r.to_latex(buf = filePath+'confu_r_down', bold_rows = True,
                 caption='Confusion Matrix NB classifier, balanced romance dataset')

print('CLASSIFICATION RESULTS FOR ROMANCE REVIEWS DATASET: ')
print(classification_report(labels_r, pred_r, digits = 4))
report_nb_r = classification_report(labels_r, pred_r, digits = 4, output_dict=True)
df_report_nb_r = pd.DataFrame(report_nb_r).transpose()
df_report_nb_r.to_latex(buf = filePath+'nb_report_r', header = ['Precision', 'Recall', 'F1-score', 'Support'],
                      bold_rows = False, float_format="%.4f",
                      caption='Classification Report NB classifier, balanced romance dataset', label='report_nb_r')
print('\nConfusion matrix:')
display(confu_r)

# science fiction books
n = 5000
a = 0.95
labels_s, pred_s = eval_nb_bi(n, a, unique_science, unigrams_science,
                              ['science_fiction_female', 'science_fiction_male'], mutual_info_science,
                              test_science, prob_class_science)

confu_s = confu_matrix(['science_fiction_female', 'science_fiction_male'], labels_s, pred_s)
confu_s.to_latex(buf = filePath+'confu_s_down', bold_rows = True,
                 caption='Confusion Matrix NB classifier, balanced science dataset')

print('CLASSIFICATION RESULTS FOR SCIENCE FICTION REVIEWS DATASET: ')
print(classification_report(labels_s, pred_s, digits = 4))
report_nb_s = classification_report(labels_s, pred_s, digits = 4, output_dict=True)
df_report_nb_s = pd.DataFrame(report_nb_s).transpose()
df_report_nb_s.to_latex(buf = filePath+'nb_report_s', header = ['Precision', 'Recall', 'F1-score', 'Support'],
                      bold_rows = False, float_format="%.4f",
                      caption='Classification Report NB classifier, balanced science dataset', label='report_nb_s')
print('\nConfusion matrix:')
display(confu_s)